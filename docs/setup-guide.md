# ğŸ§  Setup Guide â€“ Local Ollama AI Environment

This guide explains how to install and run the Ollama AI server locally to power language model inference (like llama2) on your own machine.

## âœ… Prerequisites

- Ubuntu Linux (VM or host)
- At least 8GB RAM, 10GB disk
- Python 3.10+ and `venv`
- Git installed

## ğŸ“¦ Installation

Run the installer script:

```bash
./scripts/install-ollama.sh
```

This will install Ollama to `/usr/local/bin/ollama`.

## â¬‡ï¸ Download Model

Use the provided helper script:

```bash
./models/download-models.sh
```

Default model: `llama2`

## ğŸš€ Start Ollama Server

Launch in background:

```bash
./scripts/start-ollama.sh
```

Server will run at:  
`http://localhost:11434`

## ğŸ” Test the API

Using bash:

```bash
./scripts/test-ollama.sh
```

Or with Python:

```bash
python examples/basic_call.py
```

## ğŸ›‘ Stop the Server

To stop Ollama when you're done:

```bash
./scripts/stop-ollama.sh
```

---

## ğŸ“‚ Project Structure

```
ollama-local-ai/
â”œâ”€â”€ scripts/         # install/start/stop/test
â”œâ”€â”€ models/          # model downloads
â”œâ”€â”€ examples/        # Python test + results
â”œâ”€â”€ docs/            # setup + troubleshooting
â”œâ”€â”€ config/          # optional configs
```